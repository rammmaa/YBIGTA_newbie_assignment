# 7(1)-LLM Report

---

## 1. 실험 결과: 정답률(Accuracy) 비교

| 프롬프팅 기법       | 0-shot | 3-shot | 5-shot |
| ------------------- | ------ | ------ | ------ |
| **Direct Prompting** | 20.0%  | 22.0%  | 24.0%  |
| **CoT Prompting**    | 62.0%  | 68.0%  | 74.0%  |
| **My Prompting**     | 70.0%  | 72.0%  | 66.0%  |

---

## 2. 성능 분석

### 2.1. CoT Prompting과 Direct Prompting 비교

Chain of Thought(CoT) Prompting은 Direct Prompting에 비해 모든 shot 설정에서 월등히 높은 성능을 보였다.

- Direct Prompting은 모델에게 단순히 (문제, 정답) 쌍만을 제공한다. 이는 모델이 문제 해결을 위한 방법이나 과정을 학습할 기회를 제공하지 못한다. 특히 여러 단계를 거쳐야 하는 수학 문제에서는 이 한계가 명확히 드러난다. 반면 CoT Prompting은 문제, 풀이 과정, 정답의 완전한 예시를 제공한다. 그러므로 복잡한 문제를 논리적인 단계로 나누어 해결하는 방법을 내재화할 수 있다. 따라서 단순히 정답을 추측하는 것이 아니라, 추론을 통해 정답에 도달하기 때문에 정확도가 비약적으로 상승한다.

### 2.2. My Prompting과 CoT Prompting 비교

My Prompting은 0-shot과 3-shot 설정에서 CoT와 비슷하거나 다소 높은 성능을 보였으나, 5-shot에서는 오히려 성능이 하락하는 예상 밖의 결과를 보였다.

- **가설 1: My Prompting의 효과 (0-shot, 3-shot)**  
  My Prompting은 CoT의 장점을 기반으로 명시적인 역할 부여("You are a brilliant mathematician")와 단계적 사고 지시("thinking step by step")를 추가한 전략이다. 이 두 요소가 모델에게 더 강력한 가이드라인을 제공하여, 적은 수의 예시(0~3개)만으로도 문제 해결의 방향성을 빠르고 효과적으로 설정하게 만들었다고 판단한다. 특히 예시가 없는 0-shot 환경에서 CoT보다 높은 성능을 보인 점은 명시적인 지시가 few-shot 예시의 부재를 어느 정도 보완했음을 시사한다.

- **가설 2: 성능 하락의 원인 (5-shot)**  
  5-shot 설정에서는 이미 5개의 상세한 풀이 과정 예시(CoT)가 제공된다. 여기에 역할 부여와 단계적 사고 지시라는 추가 정보가 더해지면서 프롬프트 전체 길이가 지나치게 길고 복잡해졌을 가능성이 크다. 모델이 처리해야 할 정보량이 과도해지면 핵심적인 풀이 패턴 학습에 방해가 되어 성능이 저하될 수 있다. 이는 ‘Context-stuffing’ 문제와 유사하다.  
  또한 무작위로 선택된 예제들이 특정 유형에 편중되었거나 덜 일반적이었을 가능성도 존재한다. 이 경우 강력한 지시어와 편향된 예제가 결합되어 모델 추론을 잘못된 방향으로 유도했을 수 있다.

---

## 3. 결론 및 제언

- 복잡한 추론 문제에서 CoT Prompting은 Direct Prompting보다 압도적으로 효과적이다. 이는 모델에게 ‘생각하는 방법’을 알려주는 것이 얼마나 중요한지를 명확히 보여준다.

- 역할 부여와 명시적 지시를 추가하는 My Prompting은 적은 예시(few-shot) 환경에서 CoT 성능을 더욱 향상시켰다.

- 그러나 few-shot 예시가 많아질 경우, 프롬프트의 복잡성과 정보량이 오히려 성능에 부정적인 영향을 미칠 수 있음을 확인하였다. 이는 프롬프트 엔지니어링이 단순히 정보를 많이 추가하는 것이 아니라 ‘최적의 정보량’을 찾는 과정임을 시사한다.

따라서 본 과제에서는 가장 안정적으로 높은 성능을 보인 CoT Prompting(5-shot)을 최적 프롬프팅 기법으로 추천한다.