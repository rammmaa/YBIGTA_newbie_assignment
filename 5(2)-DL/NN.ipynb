{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Instructions:*\n",
    "- **과제 명세서를 읽어주시고 코드 작성을 해주시길 바랍니다**</span> \n",
    "- **명시된 step을 따라가며 전체적인 학습 방법을 숙지합니다**</span>\n",
    "- (**첫 번째 cell 결과로 나온 시간을 기준으로 채점을 하겠습니다**</span>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2025-07-27 15:53:02.125903\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Mutilayer Perceptron(```class MutiLayerPerceptron```)으로 간단한 Binary classification task를 진행해볼 것입니다. \n",
    "\n",
    "> 1. **Dataset**\n",
    ">> $\\texttt{moon}$ dataset\n",
    "> 2. **Network architecture**\n",
    "\n",
    " > $H_1 = X \\cdot W_1 + b_1$   \n",
    " > $z_1 = ReLU(H_1)$ where $ReLU$($=\\max(0,x)$) is a rectified linear unit and $z_1$ is an output of the first hidden layer.  \n",
    " > $H_2 = z_1 \\cdot W_2 + b_2$   \n",
    " > $z_2 = LeakyReLU(H_2)$ where $LeakyReLU$($=\\max(0.01x,x)$) and $z_2$ is an output of the second hidden layer. \n",
    " > $H_3 = z_2 \\cdot W_3 + b_3$   \n",
    " > $z_3 = tanh(H_3 + H_1)$ where $\\tanh$ is a tanh function and $z_3$ is an output of the third hidden layer.  \n",
    " > $H_4 = z_3 \\cdot W_4 + b_4$   \n",
    " > $\\hat y = \\sigma(H_4)$ where $\\sigma$ is a sigmoid function unit and $\\hat y$ is an output of the network.\n",
    " \n",
    " > **$W$** and **$b$**는 각각 weights와 bias.    \n",
    " > **weight 초기화**: Standard normal ($\\texttt{np.random.randn}$. 사용)  \n",
    " > **bias 초기화(intercept)**: 0     \n",
    " > **Input size**: 2  \n",
    " > **The first hidden layer size**: 10  \n",
    " > **The second hidden layer size**: 10  \n",
    " > **Output size**: 1   \n",
    " > **Regularization parameter $\\lambda$**: 0.001  \n",
    " > **Loss function**: Binary cross entropy loss (or equivently log loss).  \n",
    " > **Total loss** : \n",
    " > $L_{total} = \\sum_{i=1}^N{ (-y^{(i)}\\log \\hat{y}^{(i)} -(1-y^{(i)})\\log(1-\\hat{y}^{(i)})) } +  \\lambda \\|W\\|^2 $   \n",
    " > **Optimization**: Gradient descent  \n",
    " > **Learning rate** = 0.0001  \n",
    " > **Number of epochs** = 50000  \n",
    " > $y$는 정답, $\\hat{y}$는 예측값이고 0부터 1사이에 존재한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "from mlp import MultiLayerPerceptron\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Load data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x188bacc20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"STEP 1: Load data\")\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = sklearn.datasets.make_moons(300, noise = 0.25)\n",
    "\n",
    "# Visualize data\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s = 40, c=y_train, cmap=plt.cm.RdYlGn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Train the model\n",
      "Loss (epoch 1000): 38.479241628853195\n",
      "Loss (epoch 2000): 31.496228930605113\n",
      "Loss (epoch 3000): 29.65075575679694\n",
      "Loss (epoch 4000): 28.66230475822087\n",
      "Loss (epoch 5000): 27.90815037233335\n",
      "Loss (epoch 6000): 26.59958282092891\n",
      "Loss (epoch 7000): 26.183074106954862\n",
      "Loss (epoch 8000): 25.818039495188962\n",
      "Loss (epoch 9000): 25.46009676223342\n",
      "Loss (epoch 10000): 25.02956193369736\n",
      "Loss (epoch 11000): 24.65797421973048\n",
      "Loss (epoch 12000): 24.35716746077572\n",
      "Loss (epoch 13000): 24.018948454313684\n",
      "Loss (epoch 14000): 23.389940833192014\n",
      "Loss (epoch 15000): 23.165247019475878\n",
      "Loss (epoch 16000): 22.984494383258852\n",
      "Loss (epoch 17000): 22.786629124126346\n",
      "Loss (epoch 18000): 22.613092998704538\n",
      "Loss (epoch 19000): 22.453088236435622\n",
      "Loss (epoch 20000): 22.322674828690744\n",
      "Loss (epoch 21000): 22.163308225860874\n",
      "Loss (epoch 22000): 22.031098982484657\n",
      "Loss (epoch 23000): 21.89992441470313\n",
      "Loss (epoch 24000): 21.77284258782966\n",
      "Loss (epoch 25000): 21.712088335883735\n",
      "Loss (epoch 26000): 21.5992360391831\n",
      "Loss (epoch 27000): 21.3734200067697\n",
      "Loss (epoch 28000): 21.12071488525356\n",
      "Loss (epoch 29000): 20.912405776475428\n",
      "Loss (epoch 30000): 20.693932877978842\n",
      "Loss (epoch 31000): 20.524844528790734\n",
      "Loss (epoch 32000): 20.416724738853944\n",
      "Loss (epoch 33000): 20.31577944352233\n",
      "Loss (epoch 34000): 20.23206657502403\n",
      "Loss (epoch 35000): 20.169316858828708\n",
      "Loss (epoch 36000): 20.13790215800112\n",
      "Loss (epoch 37000): 20.07548036364653\n",
      "Loss (epoch 38000): 20.03679579824938\n",
      "Loss (epoch 39000): 19.987003898318676\n",
      "Loss (epoch 40000): 19.87402767082343\n",
      "Loss (epoch 41000): 19.80813651600249\n",
      "Loss (epoch 42000): 19.72224701886779\n",
      "Loss (epoch 43000): 19.51823802341141\n",
      "Loss (epoch 44000): 19.268453703652867\n",
      "Loss (epoch 45000): 18.968378759833648\n",
      "Loss (epoch 46000): 18.597964110301135\n",
      "Loss (epoch 47000): 18.286958057782932\n",
      "Loss (epoch 48000): 18.02612307226678\n",
      "Loss (epoch 49000): 17.801789161076428\n",
      "Loss (epoch 50000): 17.605793875835005\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP 2: Train the model\")\n",
    "# random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "nn_input_dim = 2\n",
    "nn_output_dim = 1\n",
    "nn_hdim1 = 10\n",
    "nn_hdim2 = 10\n",
    "nn_hdim3 = 10\n",
    "lr = 0.0001 \n",
    "L2_norm = 0.001\n",
    "epoch = 50000\n",
    "\n",
    "model = MultiLayerPerceptron(nn_input_dim, nn_hdim1, nn_hdim2, nn_hdim3, nn_output_dim, init=\"random\")\n",
    "stats = model.train(X_train, y_train, learning_rate=lr, L2_norm=L2_norm, epoch=epoch, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Plot decision boundary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Decision Boundary: Hidden layer dimension (10, 10)')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"STEP 3: Plot decision boundary\")\n",
    "# Plot the decision boundary\n",
    "utils.plot_decision_boundary(lambda x: model.predict(x), X_train, y_train)\n",
    "plt.title(f\"Decision Boundary: Hidden layer dimension {nn_hdim1, nn_hdim2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(stats['loss_history'])) * 1000, stats['loss_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss over epoch')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(stats['train_acc_history'])) * 1000, stats['train_acc_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.title('Training accuracy over epoch')\n",
    "plt.gcf().set_size_inches(20, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
